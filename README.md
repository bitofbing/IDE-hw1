# IDE-hw1
智能数据工程第一次作业

## 一、从files文件中提取PPT文本数据
执行[ernie_text.py](ernie_text.py)ernie_text.py脚本，使用ernie预处理语言模型匹配PPT课件中的实体关系模型
### 1.1 部分结果展示
#### [第1章_关系数据库查询优化.pptx](files/%E7%AC%AC1%E7%AB%A0_%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96.pptx)第1章_关系数据库查询优化.pptx

| 优化后的关系表述 | 原文内容 | 来源 | 含义解释 |
|----------------|----------|------|----------|
| **查询优化器 → 需要 → 估计数据对象的大小分布情况** | "估计查询涉及数据对象的大小分布情况（查询结果大小、可能取值频率分布）" | 基数估计概述 (1) | 查询优化需要预测查询结果的数据量及其取值分布，以选择最优执行计划 |
| **基数估计 → 等价于 → 规模分布估计器** | "基数估计和规模分布估计器有什么关系？等价" | 基数估计概述 (2) | 基数估计的本质就是预测数据规模分布，两者是同一概念的不同表述 |
| **采样结果 → 通过缩放 → 得到基数估计值** | "根据在采样集上执行查询得到的结果大小除以相应缩放比例得到基数估计结果" | 传统基数估计方法 (1) | 通过对样本数据执行查询并按比例放大结果，估算全量数据的基数 |
| **目标列 → 使用 → 部分记录进行统计分析** | "目标列中用于统计分析的记录占总记录数的百分比" | 传统基数估计方法 (2) | 统计时只需分析部分数据（如10%），而非处理全部数据以提高效率 |
| **直方图类型 → 匹配 → 数据分布特征** | "不同类型的直方图对应不同的基数估计方法，根据数据列的不同分布情况选择适合的直方图" | 传统基数估计方法 (2) | 针对均匀分布、偏态分布等不同数据特征，需选用合适的直方图类型 |
| **采样方法 → 用于 → 缩减查询数据规模** | "使用采样的方法对查询数据进行缩放" | 传统基数估计方法 (8) | 通过采样降低数据处理量，再按比例还原估算值，平衡精度与性能 |
| **摘要信息 → 通过连接筛选 → 生成基数估计** | "通过对摘要信息进行连接和筛选得到采样基数值" | 传统基数估计方法 (8) | 在采样数据上模拟实际的连接和过滤操作，生成近似的基数估计 |
| **多表查询 → 需要 → 单独采样各表** | "单独对每个表进行采样，再将采样结果连接并进行查询并缩放" | 传统基数估计方法 (8) | 处理多表连接查询时，需要对每个表独立采样后再合并处理 |
| **第一个乘积项 → 对应 → 神经网络输入特征** | "第一个乘积项对应的神经网络输入是什么？" | 基于机器学习的基数估计 (7) | 在神经网络模型中，首个计算项代表输入特征的编码表示（如one-hot向量） |
| **基数估计 → 支撑 → 查询优化器决策** | "基数估计用于查询优化器的规模分布估计器" | 总结 | 通过预测数据分布情况，为查询优化器选择高效执行计划提供依据 |

#### [第2章_信息检索.pptx](files/%E7%AC%AC2%E7%AB%A0_%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2.pptx)第2章_信息检索.pptx
| 优化后的关系表述 | 原文内容 | 来源 | 含义解释 |
|----------------|----------|------|----------|
| **文档向量 → 包含 → 位置值** | "文档d<sub>i</sub>的向量在位置j上的值为x" | 信息检索模型 (1) | 向量空间模型中用向量位置表示词在文档中的出现情况 |
| **零值 → 表示 → 词未出现** | "值为0表示该词没有在该文档中出现" | 信息检索模型 (1) | 向量中0值表示对应词汇在文档中不存在 |
| **词k<sub>i</sub> → 具有 → 局部权值** | "f<sub>ij</sub>=freq<sub>ij</sub>/max tf<sub>j</sub> — 第i个词在第j个文档中的权值" | 信息检索模型 (2) | 词频权重计算公式，反映词在单个文档中的重要性 |
| **词k<sub>i</sub> → 具有 → 全局权值** | "idf<sub>i</sub>=log(n/n<sub>i</sub>) — 第i个词在整个文档集中的权值" | 信息检索模型 (2) | 逆文档频率权重，反映词在整个文档集的区分度 |
| **词k<sub>i</sub> → 出现于 → 文档d<sub>j</sub>** | "freq<sub>ij</sub> — k<sub>i</sub>在d<sub>j</sub>中出现的次数" | 信息检索模型 (3) | 记录特定词汇在特定文档中的出现频率 |
| **最大词频 → 用于 → 消除长度偏差** | "max tf<sub>j</sub> — 消除文档长度对词权的影响" | 信息检索模型 (3) | 通过归一化处理消除文档长度对词频统计的偏差 |
| **词"agent" → 出现于 → 文档d<sub>1</sub>** | "索引词k<sub>1</sub>(agent)在文档d<sub>1</sub>中出现的次数freq<sub>11</sub>=2" | 信息检索模型 (4) | 具体示例展示词频统计方法 |
| **文本"人工智能正" → 分词为 → "改变世界"** | "人工智能正在改变世界 → 人工智能，改变，世界" | 文本信息检索 (1) | 中文分词示例展示停用词去除和名词识别 |
| **文本信息检索 → 主要处理 → 文本数据** | "文本信息检索主要对象是文本数据" | 文本信息检索 (1) | 说明文本检索的核心处理对象类型 |
| **齐普夫定律 → 描述 → 词频-排名关系** | "词频f=C/r<sup>β</sup>，揭示词频和词排名之间的关系" | 文本信息检索 (1) | 描述词汇分布规律的数学模型 |
| **查询词 → 映射到 → 倒排表** | "提问式q中所有词对应的倒排表" | 文本信息检索 (2) | 倒排索引检索的核心操作步骤 |
| **倒排索引 → 记录 → 词项位置** | "获得词的文档号及在文档中出现的位置" | 文本信息检索 (3) | 倒排索引构建的关键信息要素 |
| **政府计划 → 包含动作 → 增加预算** | "政府计划增加教育预算 → 政府、计划、增加、教育、预算" | Web信息检索 (2) | 网页去重中的特征提取示例 |
| **高链接数 → 导致 → 网页受关注** | "若一个网页被较多的其他网页链接，则它相对较被人关注" | Web信息检索 (4) | PageRank算法的基本假设 |
| **游走概率 → 决定 → 节点排序** | "多轮游走后，依据概率对剩余节点进行排序" | Web信息检索 (6) | 个性化PageRank的排序机制 |
| **权威网页 → 提供 → 高质量内容** | "权威性网页对于检索而言是高质量的内容网页" | Web信息检索 (7) | HITS算法中对权威网页的定义 |

#### [第3章_数据组织.pptx](files/%E7%AC%AC3%E7%AB%A0_%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87.pptx)第3章_数据组织.pptx
| 优化后的关系表述 | 原文内容 | 来源 | 含义解释 |
|----------------|----------|------|----------|
| **数据组织 → 通过 → 规则归并数据** | "按照一定的方式和规则对数据进行归并" | 数据组织概述 (1) | 数据组织的核心处理流程 |
| **操作型系统 → 对比 → 信息型系统** | "操作型与信息型系统对比" | 数据仓库 (2) | 数据仓库与传统系统的区别 |
| **数据仓库 → 包含 → 数据集市** | "数据仓库与数据集市对比" | 数据仓库 (7) | 数据仓库的组成架构 |
| **数据仓库 → 区别于 → 数据湖** | "数据湖与数据仓库区别？" | 数据湖 (2) | 两种存储架构的差异 |
| **原始数据 → 存储于 → 初始数据池** | "将摄取的所有原始数据存储在初始数据池中" | 数据湖 (5) | 数据湖的基础存储层 |
| **初始区域 → 保存 → 原始数据** | "存储初始区域中原始数据" | 数据湖 (7) | 数据湖的分区管理 |
| **Hudi查询 → 产生 → 时间戳结果** | "Hudi三种查询方式在不同时间戳下的查询结果" | 数据湖 (9) | 数据湖的查询机制 |
| **KD树 → 用于 → 多维搜索** | "用于多维空间关键数据的搜索" | 向量数据库 (2) | 向量索引技术的应用 |
| **LSH → 实现 → 近似最近邻** | "用于高维空间数据近似最近邻搜索" | 向量数据库 (4) | 哈希索引技术的功能 |
| **HNSW → 基于 → 图索引** | "一种基于图的索引技术" | 向量数据库 (5) | 图索引技术的实现基础 |
| **高效搜索 → 伴随 → 内存降低** | "在保持搜索效率的同时降低内存消耗" | 向量数据库 (5) | 性能与资源的平衡 |
| **KD树构建 → 递归 → 维度划分** | "递归地在每个子集中选择新的维度进行划分" | 向量数据库 (3) | 索引构建算法流程 |
| **数据点相近 → 导致 → 哈希值相近** | "若两个数据点在原始空间中相近，则其哈希值也相近" | 向量数据库 (4) | LSH算法原理 |
| **参数设置 → 影响 → 系统性能** | "哈希函数的选择和阈值的设定对性能影响较大" | 向量数据库 (4) | 系统调优关键因素 |
| **精度提升 → 需要 → 效率牺牲** | "有没有牺牲效率来提高精度的方法？" | 向量数据库 (6) | 性能权衡问题 |
| **Milvus → 适用于 → 视觉识别** | "非常适合应用于包括图像和视频识别" | 向量数据库 (7) | 向量数据库应用场景 |
| **查询功能 → 支持 → 标量过滤** | "支持基于布尔表达式的标量过滤" | 向量数据库 (7) | 数据库查询特性 |
| **NLP技术 → 应用于 → AI场景** | "自然语言处理和推荐系统在内的多种人工智能场景" | 向量数据库 (9) | 技术应用领域 |
| **Pinecone → 服务 → 电商领域** | "广泛应用于电商、新闻、社交等领域" | 向量数据库 (8) | 行业解决方案 |
| **缓存机制 → 减少 → 重复计算** | "通过智能缓存机制来减少重复计算" | 向量数据库 (10) | 性能优化手段 |
| **Qdrant → 确保 → AI兼容性** | "确保在不同人工智能应用中实现准确性" | 向量数据库 (10) | 技术兼容性保证 |
| **水平扩展 → 保持 → 稳定性能** | "确保在大数据量时仍能保持稳定的性能" | 向量数据库 (11) | 系统扩展能力 |
| **相似度计算 → 实现 → 实时检索** | "基于向量相似度计算实现了实时相似度检索" | 向量数据库 (11) | 核心算法功能 |
| **Faiss → 适用于 → 相似度场景** | "使其能广泛应用于各种需要计算向量相似度的场景" | 向量数据库 (12) | 技术普适性 |
| **Faiss → 针对 → 密集向量** | "一个针对密集向量集合进行高效相似度搜索" | 向量数据库 (12) | 技术定位 |
| **数据集市 → 派生自 → 数据仓库** | "可在逻辑或物理上将其拆分为数据集市" | 数据仓库 (5) | 数据架构关系 |

#### [第4章_高维数据挖掘.pptx](files/%E7%AC%AC4%E7%AB%A0_%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98.pptx)第4章_高维数据挖掘.pptx
| 优化后的关系表述 | 原文内容 | 来源 | 含义解释 |
|----------------|----------|------|----------|
| **特征分析 → 用于 → 客户分类** | "如何对客户的某些特征进行分类" | 引例 (4) | 高维数据挖掘的应用场景 |
| **高维数据 → 导致 → 维度灾难** | "高维数据存在维度灾难问题" | 高维数据挖掘概述 (1) | 高维数据的核心挑战 |
| **降维技术 → 发现 → 低维特征** | "找出隐蔽在高维观测数据中有意义的低维向量" | 高维数据挖掘概述 (1) | 降维的核心目标 |
| **非线性降维 → 基于 → 线性技术** | "非线性降维技术通常基于线性降维技术" | 高维数据挖掘概述 (2) | 技术演进关系 |
| **线性降维 → 局限 → 保持非线性** | "线性降维通常不能在降维过程中较好地保持非线性特性" | 高维数据挖掘概述 (2) | 技术局限性 |
| **聚类算法 → 划分 → 互斥子集** | "将一组给定的数据对象划分为多个互不相交的子集" | 高维数据挖掘概述 (3) | 聚类的基本定义 |
| **簇内对象 → 具有 → 高相似度** | "簇内数据对象之间相似度高" | 高维数据挖掘概述 (3) | 聚类质量指标 |
| **簇间对象 → 具有 → 高差异性** | "簇间数据对象之间差异性大" | 高维数据挖掘概述 (3) | 聚类质量指标 |
| **距离度量 → 定义 → 对象相似性** | "定义数据对象之间的相似度" | 高维数据挖掘概述 (5) | 聚类基础方法 |
| **降维处理 → 实现 → 存储压缩** | "数据压缩减少存储空间" | 数据降维 (1) | 降维的实用价值 |
| **编码器 → 完成 → 维度压缩** | "将原始输入映射为低维数据" | 数据降维 (2) | 自编码器核心功能 |
| **解码器 → 完成 → 数据重构** | "将低维数据映射成高维数据" | 数据降维 (3) | 自编码器核心功能 |
| **贝叶斯分类 → 计算 → 类别概率** | "分类时对每个类别计算P(c_k)P(x_i|c_k)" | 数据分类 (1) | 算法核心步骤 |
| **超平面 → 存在于 → 低维空间** | "比所在数据空间小一维的空间" | 数据分类 (10) | SVM几何概念 |
| **SVM → 寻找 → 分类超平面** | "在样本空间中找出一个超平面来对数据进行分类" | 数据分类 (10) | 算法核心目标 |
| **原始空间 → 可能缺乏 → 可分超平面** | "原始样本空间可能不存在能正确划分两类样本的超平面" | 数据分类 (16) | 核函数必要性 |
| **高维空间 → 等价于 → 低维非线性** | "在高维空间解决线性问题等价于在低维空间中解决非线性问题" | 数据分类 (16) | 核函数原理 |
| **层次聚类 → 初始化 → 单对象簇** | "先将每个数据对象作为一个聚类簇" | 数据聚类 (2) | 算法初始步骤 |
| **层次聚类 → 初始化 → 全局簇** | "先将所有数据对象看作一个聚类簇" | 数据聚类 (2) | 算法初始步骤 |
| **终止条件 → 满足 → 单对象或目标** | "每个簇中只包含一个数据对象或满足目标函数" | 数据聚类 (2) | 算法终止规则 |
| **网格聚类 → 映射 → 单元对象** | "并将数据对象映射到网格单元" | 数据聚类 (3) | 算法关键步骤 |
| **MapReduce → 扩展 → 聚类算法** | "使用MapReduce框架对传统聚类算法进行扩展" | 数据聚类 (4) | 分布式计算方法 |
| **并行计算 → 扩展 → 聚类算法** | "使用并行框架对传统聚类算法进行扩展" | 数据聚类 (4) | 高性能计算方法 |
| **目标函数 → 度量 → 簇内紧密度** | "J(C)值刻画了簇内数据对象围绕簇中心点的紧密程度" | 数据聚类 (5) | 聚类质量评估 |
| **初始中心 → 选择 → 随机对象** | "随机选择k个数据对象作为初始簇中心点" | 数据聚类 (6) | k-means关键步骤 |
| **朴素贝叶斯 → 存在 → 决策错误率** | "决策存在错误率" | 总结 (1) | 算法局限性 |
| **模型性能 → 依赖 → 参数调优** | "性能对超参数敏感" | 总结 (2) | 模型调优要求 |

#### [第5章_视觉数据分析.pptx](files/%E7%AC%AC5%E7%AB%A0_%E8%A7%86%E8%A7%89%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.pptx)第5章_视觉数据分析.pptx
| 优化后的关系表述 | 原文内容 | 来源 | 含义解释 |
|----------------|----------|------|----------|
| **边界框 → 确定 → 目标位置** | "利用矩形边界框来确定图像中目标所在的位置及大小" | 目标检测 (1) | 目标检测的基本方法 |
| **局部连接 → 减少 → 参数数量** | "层与层之间稀疏的局部连接减少了参数数量" | 目标检测 (2) | CNN架构优势 |
| **卷积核 → 计算 → 区域特征** | "卷积核上的参数和窗口对应区域内的像素值进行乘法求和运算" | 目标检测 (3) | 卷积运算原理 |
| **卷积结果 → 生成 → 特征值** | "得到输出特征图对应位置的特征值" | 目标检测 (3) | 特征图生成过程 |
| **YOLO → 结合 → 特征与定位** | "将特征提取和检测框定位两个步骤相结合" | 目标检测 (5) | 算法核心思想 |
| **NMS → 选择 → 最高置信框** | "从检测框集合中取出最大置信度对应的检测框A并作为结果输出" | YOLO算法训练 (4) | 非极大值抑制过程 |
| **网格预测 → 产生 → 最终结果** | "最终得到网格4-5所对应的检测框为最终结果" | YOLO算法示例 (3) | 检测结果确定 |
| **区域划分 → 保持 → 特征一致性** | "使得这些特征在同一区域内表现出一致性或相似性" | 图像分割 (1) | 分割基本原则 |
| **RoI对齐 → 调整 → 特征维度** | "将RoI与原特征图对齐并统一维度大小" | Mask R-CNN(3) | 特征标准化处理 |
| **流程图 → 描述 → RoI处理** | "RoI对齐流程图" | Mask R-CNN(3) | 算法可视化表示 |
| **特征提取 → 基于 → ResNet101** | "使用基于CNN实现的ResNet101提取特征" | Mask R-CNN算法示例 (2) | 骨干网络选择 |
| **初始位置 → 用于 → 持续跟踪** | "对该目标在后续视频帧中进行持续的跟踪定位" | 视频目标跟踪 (1) | 跟踪算法输入 |
| **监控系统 → 执行 → 实时跟踪** | "监控摄像头系统对人群实时监控" | 视频目标跟踪 (1) | 实际应用场景 |
| **Mask R-CNN → 应用于 → 图像分割** | "基于Mask R-CNN算法对桌面图像分割" | 总结 | 算法应用实例 |

## 二、使用mongodb实现PTT课件存储
执行[uploadToMango.py](uploadToMango.py)uploadToMango.py脚本，把文件批量上传到文档数据库mongodb，并把执行结果存储到[upload_results.json](upload_results.json)upload_results.json，从这个文件可以获取文件上传的file_id

## 三、创建mysql数据库脚本
根据一、二两个步骤，我们便可以创建mysql建表脚本[create_table_mysql.sql](sql/create_table_mysql.sql)和数据插入脚本[insert_data_mysql.sql](sql/insert_data_mysql.sql)，需要注意的是，要把步骤二得到的file_id更新到ppt_files表中，执行这两个脚本我们变完成了实体关系的结构存储。
脚本测试放置于[test.sql](sql/test.sql)脚本中


## 四、迁移实体数据到Neo4j
执行脚本[可视化neo4j.py](%E5%8F%AF%E8%A7%86%E5%8C%96neo4j.py)把实体数据迁移到Neo4j中，实现知识图谱的可视化查询。
## 测试脚本如下
#### 基础查询 - 直接关系
查询"基数估计"的直接关系（作为源头和目标）
```cypher
MATCH path = (e:Entity {name: '基数估计'})-[*1..2]-(related)
RETURN path
```
#### 扩展查询 - 区分关系方向
查询"基数估计"作为源头的关系
```cypher
MATCH (e:Entity {name: '基数估计'})-[r]->(target)
RETURN e.name AS source, r.type AS relationship, target.name AS target
```
查询"基数估计"作为目标的关系
```cypher
MATCH (source)-[r]->(e:Entity {name: '基数估计'})
RETURN source.name AS source, r.type AS relationship, e.name AS target
```
#### 完整图谱查询 - 包含多跳关系
```cypher
MATCH path = (e:Entity {name: '基数估计'})-[*1..2]-(related)
RETURN path
```